# flask_app.py - RAGå¢å¼ºç‰ˆ
from flask import Flask, render_template, request, jsonify, send_file
import json
import pandas as pd
from pathlib import Path
import os
import tempfile
import random
import re
from collections import defaultdict
import hashlib
import threading
import queue
import time
from typing import List, Dict, Tuple, Optional

app = Flask(__name__)

# ========== é…ç½®è·¯å¾„ ==========
BASE_DIR = Path(__file__).parent.absolute()
CORPUS_PATH = BASE_DIR / "data" / "raw" / "medical_corpus.json"
QUESTIONS_PATH = BASE_DIR / "data" / "raw" / "medical_questions.json"

# ========== RAGé…ç½® ==========
RAG_CONFIG = {
    'chunk_size': 500,  # æ¯ä¸ªchunkçš„å­—ç¬¦æ•°
    'chunk_overlap': 50,  # chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    'top_k_retrieval': 3,  # æ£€ç´¢è¿”å›çš„chunkæ•°é‡
    'embedding_model': 'all-MiniLM-L6-v2',  # è½»é‡çº§åµŒå…¥æ¨¡å‹
    'use_semantic_search': True,  # æ˜¯å¦ä½¿ç”¨è¯­ä¹‰æœç´¢
    'hybrid_search': True,  # æ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢
}

# ========== å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹ ==========
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    try:
        import faiss
        HAS_FAISS = True
    except ImportError:
        import subprocess, sys
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'faiss-cpu'])
        import faiss
        HAS_FAISS = True
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    print("ğŸ”„ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
    embedding_model = SentenceTransformer(RAG_CONFIG['embedding_model'])
    print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # å‘é‡å­˜å‚¨
    vector_store = {
        'corpus_chunks': [],
        'corpus_embeddings': None,
        'corpus_faiss_index': None,
        'question_embeddings': None,
        'questions': [],
        'question_faiss_index': None
    }
    
    HAS_EMBEDDING = True
except ImportError as e:
    print(f"âš ï¸  æœªå®‰è£…sentence-transformers: {e}")
    print("  ä½¿ç”¨ pip install sentence-transformers å®‰è£…")
    HAS_EMBEDDING = False
    embedding_model = None
    vector_store = None

# ========== ç¿»è¯‘é˜Ÿåˆ—ç³»ç»Ÿï¼ˆé¿å…å¡é¡¿ï¼‰ ==========
class TranslationQueue:
    def __init__(self):
        self.queue = queue.Queue()
        self.results = {}
        self.worker_thread = None
        self.start_worker()
    
    def start_worker(self):
        """å¯åŠ¨ç¿»è¯‘å·¥ä½œçº¿ç¨‹"""
        self.worker_thread = threading.Thread(target=self._translation_worker, daemon=True)
        self.worker_thread.start()
        print("âœ… ç¿»è¯‘é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
    
    def _translation_worker(self):
        """ç¿»è¯‘å·¥ä½œçº¿ç¨‹"""
        from translate import Translator
        
        # åˆ›å»ºtranslatorå®ä¾‹
        translator_en_to_zh = Translator(to_lang="zh", from_lang="en")
        translator_zh_to_en = Translator(to_lang="en", from_lang="zh")
        
        while True:
            try:
                task = self.queue.get()
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                    
                task_id, text, direction = task
                
                try:
                    if direction == 'en_to_zh':
                        result = translator_en_to_zh.translate(text)
                    else:  # zh_to_en
                        result = translator_zh_to_en.translate(text)
                    
                    self.results[task_id] = result
                except Exception as e:
                    print(f"ç¿»è¯‘å¤±è´¥ ({direction}): {e}")
                    self.results[task_id] = text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡æœ¬
                    
                self.queue.task_done()
                
            except Exception as e:
                print(f"ç¿»è¯‘å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
    
    def translate(self, text, direction='en_to_zh', timeout=10):
        """æäº¤ç¿»è¯‘ä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰"""
        if not text or not any('a' <= char.lower() <= 'z' for char in text) if direction == 'en_to_zh' else not any('\u4e00' <= char <= '\u9fff' for char in text):
            return text
        
        task_id = hashlib.md5(f"{text}_{direction}".encode()).hexdigest()
        
        # å¦‚æœå·²ç»æœ‰ç»“æœï¼Œç›´æ¥è¿”å›
        if task_id in self.results:
            return self.results[task_id]
        
        # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
        self.queue.put((task_id, text, direction))
        
        # ç­‰å¾…ç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰
        start_time = time.time()
        while task_id not in self.results:
            if time.time() - start_time > timeout:
                print(f"ç¿»è¯‘è¶…æ—¶: {text[:50]}...")
                return text  # è¶…æ—¶è¿”å›åŸæ–‡æœ¬
            time.sleep(0.1)
        
        return self.results.get(task_id, text)

# åˆå§‹åŒ–ç¿»è¯‘é˜Ÿåˆ—
try:
    from translate import Translator
    translation_queue = TranslationQueue()
    HAS_TRANSLATE = True
    print("âœ… translateåº“å·²æˆåŠŸåˆå§‹åŒ–ï¼ˆä½¿ç”¨é˜Ÿåˆ—ç³»ç»Ÿï¼‰")
except ImportError as e:
    HAS_TRANSLATE = False
    print(f"âš ï¸  translateåº“æœªå®‰è£…: {e}")
    translation_queue = None

# ========== æ–‡æ¡£å¤„ç†å‡½æ•° ==========
def split_text_into_chunks(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆchunks"""
    if not text:
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        
        # å¦‚æœä¸åœ¨æ–‡æœ¬æœ«å°¾ï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        if end < text_length:
            # æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
            sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n']
            for sep in sentence_endings:
                sep_pos = text.rfind(sep, start, end)
                if sep_pos != -1 and sep_pos > start + chunk_size // 2:
                    end = sep_pos + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # ç§»åŠ¨å¼€å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
        start = end - chunk_overlap
    
    return chunks

def create_corpus_chunks(corpus_data: Dict) -> List[Dict]:
    """åˆ›å»ºè¯­æ–™åº“chunks"""
    if not corpus_data or 'full_content' not in corpus_data:
        return []
    
    full_content = corpus_data['full_content']
    raw_chunks = split_text_into_chunks(
        full_content, 
        RAG_CONFIG['chunk_size'], 
        RAG_CONFIG['chunk_overlap']
    )
    
    chunks = []
    for i, chunk_text in enumerate(raw_chunks):
        chunks.append({
            'id': f'chunk_{i:04d}',
            'text': chunk_text,
            'char_count': len(chunk_text),
            'word_count': len(chunk_text.split()),
            'chunk_index': i,
            'source': 'corpus'
        })
    
    print(f"ğŸ“„ å·²å°†è¯­æ–™åº“åˆ†å‰²æˆ {len(chunks)} ä¸ªchunks")
    return chunks

# ========== å‘é‡åŒ–å‡½æ•° ==========
def compute_embeddings(texts: List[str]) -> np.ndarray:
    """è®¡ç®—æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
    if not HAS_EMBEDDING or not embedding_model:
        return None
    
    try:
        # æ‰¹é‡è®¡ç®—åµŒå…¥
        embeddings = embedding_model.encode(texts, show_progress_bar=False)
        return embeddings
    except Exception as e:
        print(f"è®¡ç®—åµŒå…¥å¤±è´¥: {e}")
        return None

def build_vector_store(corpus_data: Dict, questions_data: Dict):
    """æ„å»ºå‘é‡å­˜å‚¨ï¼ˆå«faissç´¢å¼•ï¼‰"""
    if not HAS_EMBEDDING:
        return
    print("ğŸ”„ æ­£åœ¨æ„å»ºå‘é‡å­˜å‚¨...")
    # å¤„ç†è¯­æ–™åº“
    if corpus_data:
        corpus_chunks = create_corpus_chunks(corpus_data)
        if corpus_chunks:
            chunk_texts = [chunk['text'] for chunk in corpus_chunks]
            corpus_embeddings = compute_embeddings(chunk_texts)
            vector_store['corpus_chunks'] = corpus_chunks
            vector_store['corpus_embeddings'] = corpus_embeddings
            # æ„å»ºfaissç´¢å¼•
            if HAS_FAISS and corpus_embeddings is not None:
                dim = corpus_embeddings.shape[1]
                index = faiss.IndexFlatL2(dim)
                index.add(np.array(corpus_embeddings, dtype=np.float32))
                vector_store['corpus_faiss_index'] = index
            print(f"   âœ“ è¯­æ–™åº“å‘é‡: {len(corpus_chunks)} chunks")
    # å¤„ç†é—®é¢˜
    if questions_data and 'all_questions' in questions_data:
        questions = []
        for q in questions_data['all_questions']:
            question_text = q.get('raw_question', '')
            answer_text = q.get('raw_answer', '')
            combined_text = f"é—®é¢˜: {question_text}\nç­”æ¡ˆ: {answer_text}"
            questions.append(combined_text)
        if questions:
            question_embeddings = compute_embeddings(questions)
            vector_store['questions'] = questions_data['all_questions']
            vector_store['question_embeddings'] = question_embeddings
            # æ„å»ºfaissç´¢å¼•
            if HAS_FAISS and question_embeddings is not None:
                dim = question_embeddings.shape[1]
                index = faiss.IndexFlatL2(dim)
                index.add(np.array(question_embeddings, dtype=np.float32))
                vector_store['question_faiss_index'] = index
            print(f"   âœ“ é—®é¢˜å‘é‡: {len(questions)} ä¸ªé—®é¢˜")
    print("âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ")

# ========== æ£€ç´¢å‡½æ•° ==========
def semantic_search(query: str, embeddings: np.ndarray, texts: List[Dict], top_k: int = 3) -> List[Dict]:
    """è¯­ä¹‰æœç´¢ï¼ˆfaissåŠ é€Ÿï¼‰"""
    if not HAS_EMBEDDING or embeddings is None:
        return []
    try:
        query_embedding = embedding_model.encode([query])[0]
        if HAS_FAISS and vector_store.get('corpus_faiss_index') is not None:
            D, I = vector_store['corpus_faiss_index'].search(np.array([query_embedding], dtype=np.float32), top_k)
            results = []
            for idx, dist in zip(I[0], D[0]):
                if idx < len(texts):
                    results.append({
                        'text': texts[idx]['text'] if isinstance(texts[idx], dict) else texts[idx],
                        'metadata': texts[idx] if isinstance(texts[idx], dict) else {},
                        'similarity': float(-dist),
                        'source': 'semantic_search'
                    })
            return results
        else:
            # fallback: numpy
            similarities = np.dot(embeddings, query_embedding) / (
                np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            results = []
            for idx in top_indices:
                if idx < len(texts):
                    results.append({
                        'text': texts[idx]['text'] if isinstance(texts[idx], dict) else texts[idx],
                        'metadata': texts[idx] if isinstance(texts[idx], dict) else {},
                        'similarity': float(similarities[idx]),
                        'source': 'semantic_search'
                    })
            return results
    except Exception as e:
        print(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
        return []

def keyword_search(query: str, texts: List[Dict], top_k: int = 3) -> List[Dict]:
    """å…³é”®è¯æœç´¢"""
    query_terms = query.lower().split()
    scored_texts = []
    
    for i, text_item in enumerate(texts):
        if isinstance(text_item, dict):
            text = text_item.get('text', '')
            metadata = text_item
        else:
            text = text_item
            metadata = {'text': text}
        
        text_lower = text.lower()
        score = 0
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        for term in query_terms:
            if term in text_lower:
                score += 1
            # éƒ¨åˆ†åŒ¹é…
            if len(term) > 3 and any(term in word for word in text_lower.split()):
                score += 0.5
        
        if score > 0:
            scored_texts.append({
                'text': text,
                'metadata': metadata,
                'score': score,
                'source': 'keyword_search'
            })
    
    # æŒ‰åˆ†æ•°æ’åº
    scored_texts.sort(key=lambda x: x['score'], reverse=True)
    return scored_texts[:top_k]

def hybrid_retrieval(query: str, corpus_data: Dict, questions_data: Dict, top_k: int = 3) -> List[Dict]:
    """æ··åˆæ£€ç´¢ï¼šç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢"""
    all_results = []
    
    # 1. ä»è¯­æ–™åº“æ£€ç´¢
    if vector_store and vector_store['corpus_embeddings'] is not None:
        semantic_results = semantic_search(
            query, 
            vector_store['corpus_embeddings'],
            vector_store['corpus_chunks'],
            top_k=top_k
        )
        all_results.extend(semantic_results)
    
    # 2. å…³é”®è¯æœç´¢è¯­æ–™åº“
    if corpus_data and 'paragraphs' in corpus_data:
        paragraphs = [{'text': p, 'metadata': {}} for p in corpus_data['paragraphs']]
        keyword_results = keyword_search(query, paragraphs, top_k=top_k)
        all_results.extend(keyword_results)
    
    # 3. ä»é—®é¢˜åº“æ£€ç´¢
    if RAG_CONFIG['hybrid_search'] and questions_data and 'all_questions' in questions_data:
        # ä½¿ç”¨ä¼ ç»Ÿæœç´¢å‡½æ•°
        search_results = search_in_questions(query, questions_data, answer_language='zh', top_k=top_k)
        for result in search_results:
            all_results.append({
                'text': f"{result.get('display_question', '')}\n{result.get('display_answer', '')}",
                'metadata': result,
                'similarity': result.get('confidence', 0.5),
                'source': 'question_search'
            })
    
    # å»é‡å’Œæ’åº
    unique_results = []
    seen_texts = set()
    
    for result in all_results:
        text_hash = hashlib.md5(result['text'].encode()).hexdigest()
        if text_hash not in seen_texts:
            seen_texts.add(text_hash)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            if 'similarity' in result:
                score = result['similarity']
            elif 'score' in result:
                score = result['score'] / 10  # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            else:
                score = 0.5
            
            result['confidence'] = min(score, 0.95)
            unique_results.append(result)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    unique_results.sort(key=lambda x: x.get('confidence', 0), reverse=True)
    return unique_results[:top_k]

# ========== ç­”æ¡ˆç”Ÿæˆå‡½æ•° ==========
def generate_answer_from_context(query: str, retrieved_contexts: List[Dict], answer_language: str = 'zh') -> Dict:
    """åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
    if not retrieved_contexts:
        return {
            'answer': 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚',
            'sources': [],
            'confidence': 0.0
        }
    
    # åˆå¹¶ä¸Šä¸‹æ–‡
    context_texts = []
    sources = []
    
    for i, ctx in enumerate(retrieved_contexts):
        context_text = ctx.get('text', '')
        source_info = ctx.get('metadata', {})
        confidence = ctx.get('confidence', 0.5)
        
        context_texts.append(f"[æ¥æº{i+1}, ç½®ä¿¡åº¦:{confidence:.2f}] {context_text}")
        sources.append({
            'text': context_text[:200] + "..." if len(context_text) > 200 else context_text,
            'confidence': confidence,
            'source_type': ctx.get('source', 'unknown')
        })
    
    combined_context = "\n\n".join(context_texts)
    
    # åŸºäºä¸Šä¸‹æ–‡çš„ç®€å•ç­”æ¡ˆç”Ÿæˆ
    query_lower = query.lower()
    context_lower = combined_context.lower()
    
    # å°è¯•æå–ç›´æ¥ç­”æ¡ˆ
    answer = ""
    
    # å¦‚æœæ˜¯å®šä¹‰ç±»é—®é¢˜
    if any(word in query_lower for word in ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'definition', 'what is']):
        # æå–ç¬¬ä¸€ä¸ªåŒ…å«æŸ¥è¯¢å…³é”®è¯çš„å¥å­
        sentences = re.split(r'[ã€‚.ï¼!ï¼Ÿ?\n]', combined_context)
        for sentence in sentences:
            if any(term in sentence.lower() for term in query_lower.split()):
                answer = sentence.strip()
                break
    
    # å¦‚æœæ˜¯ç—‡çŠ¶æˆ–æ²»ç–—ç±»é—®é¢˜
    elif any(word in query_lower for word in ['ç—‡çŠ¶', 'è¡¨ç°', 'symptom', 'treatment', 'æ²»ç–—', 'ç–—æ³•']):
        # æå–åŒ…å«ç›¸å…³ä¿¡æ¯çš„æ®µè½
        paragraphs = combined_context.split('\n\n')
        relevant_paras = []
        for para in paragraphs:
            para_lower = para.lower()
            if ('ç—‡çŠ¶' in para_lower or 'symptom' in para_lower) and 'æ²»ç–—' in query_lower:
                relevant_paras.append(para)
            elif ('æ²»ç–—' in para_lower or 'treatment' in para_lower) and 'æ²»ç–—' in query_lower:
                relevant_paras.append(para)
        
        if relevant_paras:
            answer = "\n".join(relevant_paras[:2])
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°ç‰¹å®šç­”æ¡ˆï¼Œä½¿ç”¨æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡
    if not answer:
        answer = retrieved_contexts[0].get('text', '')
        # æˆªå–åˆç†é•¿åº¦
        if len(answer) > 500:
            sentences = re.split(r'[ã€‚.ï¼!ï¼Ÿ?]', answer)
            answer = ""
            for sentence in sentences:
                if len(answer + sentence) < 500:
                    answer += sentence + "ã€‚"
                else:
                    break
    
    # æ¸…ç†ç­”æ¡ˆæ ¼å¼
    answer = re.sub(r'\s+', ' ', answer).strip()
    
    # æ·»åŠ æç¤ºä¿¡æ¯
    if len(answer) > 0:
        answer += "\n\nï¼ˆä»¥ä¸Šä¿¡æ¯åŸºäºåŒ»ç–—çŸ¥è¯†åº“ï¼Œä»…ä¾›å‚è€ƒã€‚å…·ä½“ç—…æƒ…è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚ï¼‰"
    
    # ç¿»è¯‘ç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰
    if answer_language == 'en':
        answer = translate_to_english_fast(answer)
    elif answer_language == 'zh':
        answer = translate_to_chinese_fast(answer)
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    avg_confidence = sum(s['confidence'] for s in sources) / len(sources) if sources else 0.5
    
    return {
        'answer': answer,
        'sources': sources,
        'confidence': avg_confidence
    }

# ========== RAGé—®ç­”å‡½æ•° ==========
def rag_query(query: str, corpus_data: Dict, questions_data: Dict, answer_language: str = 'zh') -> Dict:
    """RAGé—®ç­”ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # 1. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    retrieved_contexts = hybrid_retrieval(
        query, 
        corpus_data, 
        questions_data, 
        top_k=RAG_CONFIG['top_k_retrieval']
    )
    
    retrieval_time = time.time() - start_time
    
    # 2. ç”Ÿæˆç­”æ¡ˆ
    generation_start = time.time()
    result = generate_answer_from_context(query, retrieved_contexts, answer_language)
    generation_time = time.time() - generation_start
    
    # 3. å‡†å¤‡è¿”å›ç»“æœ
    total_time = time.time() - start_time
    
    # å‡†å¤‡æºæ–‡æ¡£ä¿¡æ¯
    source_documents = []
    for i, ctx in enumerate(retrieved_contexts[:3]):
        source_text = ctx.get('text', '')
        if len(source_text) > 150:
            source_text = source_text[:150] + "..."
        
        source_documents.append({
            'id': i + 1,
            'content': source_text,
            'confidence': ctx.get('confidence', 0.5),
            'source_type': ctx.get('source', 'unknown')
        })
    
    return {
        'answer': result['answer'],
        'confidence': result['confidence'],
        'source_documents': source_documents,
        'retrieved_count': len(retrieved_contexts),
        'timing': {
            'retrieval': f"{retrieval_time:.2f}s",
            'generation': f"{generation_time:.2f}s",
            'total': f"{total_time:.2f}s"
        },
        'used_rag': True
    }

# ========== ä¼˜åŒ–ç¿»è¯‘å‡½æ•° ==========
def translate_to_chinese_fast(text):
    """å¿«é€Ÿç¿»è¯‘æˆä¸­æ–‡ï¼ˆä½¿ç”¨ç¼“å­˜å’Œé˜Ÿåˆ—ï¼‰"""
    if not text or not isinstance(text, str):
        return text or ""
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘
    if not any('a' <= char.lower() <= 'z' for char in text):
        return text
    
    # ä½¿ç”¨ç¿»è¯‘é˜Ÿåˆ—
    if translation_queue:
        return translation_queue.translate(text, direction='en_to_zh', timeout=5)
    
    # é™çº§åˆ°ç®€æ˜“ç¿»è¯‘
    return simple_translate_to_chinese(text)

def translate_to_english_fast(text):
    """å¿«é€Ÿç¿»è¯‘æˆè‹±æ–‡ï¼ˆä½¿ç”¨ç¼“å­˜å’Œé˜Ÿåˆ—ï¼‰"""
    if not text or not isinstance(text, str):
        return text or ""
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘
    if not any('\u4e00' <= char <= '\u9fff' for char in text):
        return text
    
    # ä½¿ç”¨ç¿»è¯‘é˜Ÿåˆ—
    if translation_queue:
        return translation_queue.translate(text, direction='zh_to_en', timeout=5)
    
    # é™çº§åˆ°ç®€æ˜“ç¿»è¯‘
    return simple_translate_to_english(text)

def simple_translate_to_chinese(text):
    """ç®€æ˜“ç¿»è¯‘ï¼šè‹±æ–‡åˆ°ä¸­æ–‡ï¼ˆå¤‡ä»½æ–¹æ¡ˆï¼‰"""
    if not text:
        return text
    
    # å…³é”®åŒ»å­¦æœ¯è¯­ç¿»è¯‘
    medical_terms = {
        'skin cancer': 'çš®è‚¤ç™Œ',
        'cancer': 'ç™Œç—‡',
        'diabetes': 'ç³–å°¿ç—…',
        'high blood pressure': 'é«˜è¡€å‹',
        'pneumonia': 'è‚ºç‚',
        'heart disease': 'å¿ƒè„ç—…',
        'common cold': 'æ™®é€šæ„Ÿå†’',
        'basal cell carcinoma': 'åŸºåº•ç»†èƒç™Œ',
        'squamous cell carcinoma': 'é³çŠ¶ç»†èƒç™Œ',
        'nonmelanoma': 'éé»‘è‰²ç´ ç˜¤',
        'melanoma': 'é»‘è‰²ç´ ç˜¤',
        'CSCC': 'çš®è‚¤é³çŠ¶ç»†èƒç™Œ',
        'BCC': 'åŸºåº•ç»†èƒç™Œ',
    }
    
    result = text
    for en, zh in medical_terms.items():
        if en.lower() in result.lower():
            result = re.sub(r'\b' + re.escape(en) + r'\b', zh, result, flags=re.IGNORECASE)
    
    return result

def simple_translate_to_english(text):
    """ç®€æ˜“ç¿»è¯‘ï¼šä¸­æ–‡åˆ°è‹±æ–‡ï¼ˆå¤‡ä»½æ–¹æ¡ˆï¼‰"""
    if not text:
        return text
    
    medical_terms = {
        'çš®è‚¤ç™Œ': 'skin cancer',
        'ç™Œç—‡': 'cancer',
        'ç³–å°¿ç—…': 'diabetes',
        'é«˜è¡€å‹': 'high blood pressure',
        'è‚ºç‚': 'pneumonia',
        'å¿ƒè„ç—…': 'heart disease',
        'æ„Ÿå†’': 'common cold',
        'åŸºåº•ç»†èƒç™Œ': 'basal cell carcinoma',
        'é³çŠ¶ç»†èƒç™Œ': 'squamous cell carcinoma',
        'éé»‘è‰²ç´ ç˜¤': 'nonmelanoma',
        'é»‘è‰²ç´ ç˜¤': 'melanoma',
    }
    
    result = text
    for zh, en in medical_terms.items():
        if zh in result:
            result = result.replace(zh, en)
    
    return result

def ensure_pure_chinese(text):
    """ç¡®ä¿æ–‡æœ¬æ˜¯çº¯ä¸­æ–‡"""
    if not text:
        return text
    
    if any('a' <= char.lower() <= 'z' for char in text):
        return translate_to_chinese_fast(text)
    
    return text

def ensure_pure_english(text):
    """ç¡®ä¿æ–‡æœ¬æ˜¯çº¯è‹±æ–‡"""
    if not text:
        return text
    
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return translate_to_english_fast(text)
    
    return text

def load_corpus_data():
    """åŠ è½½è¯­æ–™åº“æ•°æ®"""
    try:
        if CORPUS_PATH.exists():
            with open(CORPUS_PATH, 'r', encoding='utf-8') as f:
                corpus = json.load(f)
            
            if isinstance(corpus, dict) and 'context' in corpus:
                context = corpus.get('context', '')
                paragraphs = [p.strip() for p in context.split('\n\n') if p.strip()]
                
                return {
                    'corpus_name': corpus.get('corpus_name', 'åŒ»ç–—çŸ¥è¯†åº“'),
                    'doc_count': 1,
                    'paragraphs': paragraphs,
                    'full_content': context
                }
        else:
            print(f"è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {CORPUS_PATH}")
        return None
    except Exception as e:
        print(f"åŠ è½½è¯­æ–™åº“å¤±è´¥: {e}")
        return None

def load_questions_data():
    """åŠ è½½é—®é¢˜é›†æ•°æ®ï¼ˆä¸è¿›è¡Œç¿»è¯‘ï¼Œå»¶è¿Ÿç¿»è¯‘ï¼‰"""
    try:
        if QUESTIONS_PATH.exists():
            with open(QUESTIONS_PATH, 'r', encoding='utf-8') as f:
                questions = json.load(f)
            
            if isinstance(questions, list):
                question_types = defaultdict(int)
                all_questions = []
                
                for q in questions:
                    if isinstance(q, dict) and 'question' in q and 'answer' in q:
                        q_type = q.get('question_type', 'å…¶ä»–')
                        question_types[q_type] += 1
                        
                        question_text = q.get('question', '')
                        answer_text = q.get('answer', '')
                        
                        # åˆ¤æ–­åŸå§‹è¯­è¨€ï¼Œä½†ä¸ç«‹å³ç¿»è¯‘
                        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in question_text)
                        
                        if has_chinese:
                            # åŸå§‹æ˜¯ä¸­æ–‡ï¼Œä¿å­˜åŸæ–‡æœ¬
                            question_cn = question_text
                            answer_cn = answer_text
                            # è‹±æ–‡ç‰ˆæœ¬å…ˆè®¾ä¸ºç©ºï¼Œéœ€è¦æ—¶å†ç¿»è¯‘
                            question_en = ""
                            answer_en = ""
                        else:
                            # åŸå§‹æ˜¯è‹±æ–‡ï¼Œä¿å­˜åŸæ–‡æœ¬
                            question_en = question_text
                            answer_en = answer_text
                            # ä¸­æ–‡ç‰ˆæœ¬å…ˆè®¾ä¸ºç©ºï¼Œéœ€è¦æ—¶å†ç¿»è¯‘
                            question_cn = ""
                            answer_cn = ""
                        
                        all_questions.append({
                            'question_cn': question_cn,
                            'question_en': question_en,
                            'answer_cn': answer_cn,
                            'answer_en': answer_en,
                            'type': q_type,
                            'source': q.get('source', 'Medical'),
                            'original_lang': 'zh' if has_chinese else 'en',
                            'raw_question': question_text,  # ä¿å­˜åŸå§‹é—®é¢˜
                            'raw_answer': answer_text,      # ä¿å­˜åŸå§‹ç­”æ¡ˆ
                        })
                
                sample_questions = all_questions[:50] if len(all_questions) > 50 else all_questions
                
                return {
                    'total_count': len(all_questions),
                    'sample_questions': sample_questions,
                    'question_types': dict(question_types),
                    'all_questions': all_questions
                }
        else:
            print(f"é—®é¢˜é›†æ–‡ä»¶ä¸å­˜åœ¨: {QUESTIONS_PATH}")
        return None
    except Exception as e:
        print(f"åŠ è½½é—®é¢˜é›†å¤±è´¥: {e}")
        return None

def get_data_counts():
    """è·å–æ•°æ®ç»Ÿè®¡"""
    global GLOBAL_CORPUS_DATA, GLOBAL_QUESTIONS_DATA
    corpus_data = GLOBAL_CORPUS_DATA
    questions_data = GLOBAL_QUESTIONS_DATA
    doc_count = corpus_data['doc_count'] if corpus_data else 1
    question_count = questions_data['total_count'] if questions_data else 0
    return doc_count, question_count, corpus_data, questions_data

# ========== æ™ºèƒ½æœç´¢å‡½æ•°ï¼ˆå»¶è¿Ÿç¿»è¯‘ï¼‰ ==========
# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æ•°æ®å’Œå‘é‡å­˜å‚¨
GLOBAL_CORPUS_DATA = None
GLOBAL_QUESTIONS_DATA = None
GLOBAL_VECTOR_STORE_READY = False

def initialize_data_and_vectors():
    """å¯åŠ¨æ—¶åŠ è½½æ•°æ®å’Œæ„å»ºå‘é‡å­˜å‚¨ï¼Œåªè¿è¡Œä¸€æ¬¡"""
    global GLOBAL_CORPUS_DATA, GLOBAL_QUESTIONS_DATA, GLOBAL_VECTOR_STORE_READY
    GLOBAL_CORPUS_DATA = load_corpus_data()
    GLOBAL_QUESTIONS_DATA = load_questions_data()
    if HAS_EMBEDDING and GLOBAL_CORPUS_DATA and GLOBAL_QUESTIONS_DATA:
        build_vector_store(GLOBAL_CORPUS_DATA, GLOBAL_QUESTIONS_DATA)
        GLOBAL_VECTOR_STORE_READY = True
    else:
        GLOBAL_VECTOR_STORE_READY = False

# å¯é€‰ï¼šæš´éœ²ä¸€ä¸ªåˆ·æ–°æ¥å£ï¼ˆå¦‚æœ‰éœ€è¦å¯æ‰‹åŠ¨åˆ·æ–°æ•°æ®å’Œå‘é‡ï¼‰
def refresh_data_and_vectors():
    initialize_data_and_vectors()
def search_in_questions(query, questions_data, answer_language='zh', top_k=5):
    """æ™ºèƒ½æœç´¢ç®—æ³•ï¼ˆå»¶è¿Ÿç¿»è¯‘ï¼‰"""
    if not questions_data or 'all_questions' not in questions_data:
        return []
    
    query = query.strip()
    if not query:
        return []
    
    # åˆ¤æ–­æŸ¥è¯¢è¯­è¨€
    has_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)
    query_lang = 'zh' if has_chinese else 'en'
    
    results = []
    
    for q in questions_data['all_questions']:
        score = 0
        
        # è·å–åŸå§‹æ–‡æœ¬
        raw_question = q.get('raw_question', '')
        raw_answer = q.get('raw_answer', '')
        original_lang = q.get('original_lang', 'en')
        
        # æ ¹æ®æŸ¥è¯¢è¯­è¨€è¿›è¡ŒåŒ¹é…ï¼ˆä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼‰
        if query_lang == 'zh':
            # ä¸­æ–‡æŸ¥è¯¢
            query_lower = query.lower()
            if original_lang == 'zh':
                # åŸå§‹æ˜¯ä¸­æ–‡ï¼Œç›´æ¥åŒ¹é…
                if query_lower in raw_question.lower():
                    score += 10
                elif any(word in raw_question.lower() for word in query_lower.split()):
                    score += 5
            else:
                # åŸå§‹æ˜¯è‹±æ–‡ï¼Œç¿»è¯‘ååŒ¹é…
                translated_question = simple_translate_to_chinese(raw_question)
                if query_lower in translated_question.lower():
                    score += 8
        else:
            # è‹±æ–‡æŸ¥è¯¢
            query_lower = query.lower()
            if original_lang == 'en':
                # åŸå§‹æ˜¯è‹±æ–‡ï¼Œç›´æ¥åŒ¹é…
                if query_lower in raw_question.lower():
                    score += 10
                elif any(word in raw_question.lower() for word in query_lower.split()):
                    score += 5
            else:
                # åŸå§‹æ˜¯ä¸­æ–‡ï¼Œç¿»è¯‘ååŒ¹é…
                translated_question = simple_translate_to_english(raw_question)
                if query_lower in translated_question.lower():
                    score += 8
        
        if score > 0:
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„å›ç­”è¯­è¨€é€‰æ‹©æ˜¾ç¤ºå†…å®¹ï¼ˆå»¶è¿Ÿç¿»è¯‘ï¼‰
            if answer_language == 'en':
                # è‹±æ–‡å›ç­”
                if original_lang == 'en':
                    display_question = ensure_pure_english(raw_question)
                    display_answer = ensure_pure_english(raw_answer)
                else:
                    display_question = translate_to_english_fast(raw_question)
                    display_answer = translate_to_english_fast(raw_answer)
            else:
                # ä¸­æ–‡å›ç­”
                if original_lang == 'zh':
                    display_question = ensure_pure_chinese(raw_question)
                    display_answer = ensure_pure_chinese(raw_answer)
                else:
                    display_question = translate_to_chinese_fast(raw_question)
                    display_answer = translate_to_chinese_fast(raw_answer)
            
            confidence = min(score / 10, 0.95)
            
            # ç¿»è¯‘ç±»å‹å’Œæ¥æº
            q_type = q.get('type', 'Medical')
            source = q.get('source', 'Medical Database')
            
            if answer_language == 'zh':
                if q_type == 'Fact Retrieval':
                    q_type = 'äº‹å®æ£€ç´¢'
                elif q_type == 'Medical':
                    q_type = 'åŒ»ç–—ä¿¡æ¯'
                if source == 'Medical Database':
                    source = 'åŒ»ç–—æ•°æ®åº“'
            
            results.append({
                'display_question': display_question,
                'display_answer': display_answer,
                'type': q_type,
                'source': source,
                'confidence': confidence,
                'original_lang': original_lang
            })
    
    # æ’åºå¹¶è¿”å›
    results.sort(key=lambda x: x['confidence'], reverse=True)
    
    # å»é‡
    unique_results = []
    seen_questions = set()
    
    for result in results:
        question_key = hashlib.md5(result['display_question'].encode()).hexdigest()
        if question_key not in seen_questions:
            seen_questions.add(question_key)
            unique_results.append(result)
        
        if len(unique_results) >= top_k:
            break
    
    return unique_results

    # ...existing code...
@app.route('/')
def index():
    """ä¸»é¡µ"""
    doc_count, question_count, corpus_data, questions_data = get_data_counts()
    sample_questions = []
    if questions_data and 'sample_questions' in questions_data:
        all_samples = questions_data['sample_questions'][:20]
        if len(all_samples) >= 3:
            sample_questions = random.sample(all_samples, 3)
        else:
            sample_questions = all_samples
    display_questions = []
    for i, sq in enumerate(sample_questions):
        question_text = sq.get('raw_question', '')
        if sq.get('original_lang') == 'en':
            display_text = simple_translate_to_chinese(question_text)
        else:
            display_text = question_text
        if len(display_text) > 40:
            display_text = display_text[:40] + "..."
        display_questions.append({
            'text': display_text,
            'full_question': question_text,
            'index': i,
            'lang': sq.get('original_lang', 'en')
        })
    return render_template('index.html',
                         doc_count=doc_count,
                         question_count=question_count,
                         sample_questions=display_questions,
                         has_rag=HAS_EMBEDDING)

@app.route('/api/query', methods=['POST'])
def handle_query():
    """å¤„ç†æŸ¥è¯¢è¯·æ±‚"""
    try:
        data = request.json
        question = data.get('question', '').strip()
        answer_language = data.get('answer_language', 'zh')
        use_rag = data.get('use_rag', True)  # æ˜¯å¦ä½¿ç”¨RAG
        
        if not question:
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥é—®é¢˜'})
        
        _, _, corpus_data, questions_data = get_data_counts()
        
        if not corpus_data or not questions_data:
            return jsonify({
                'success': False,
                'error': 'æ— æ³•åŠ è½½æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶'
            })
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨RAGé€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if use_rag and HAS_EMBEDDING:
            # ä½¿ç”¨RAG
            rag_result = rag_query(question, corpus_data, questions_data, answer_language)
            
            # ç”ŸæˆHTMLå“åº”
            answer_html = generate_rag_answer_html(question, rag_result, answer_language)
            
            return jsonify({
                'success': True,
                'question': question,
                'answer': answer_html,
                'confidence': rag_result['confidence'],
                'result_count': rag_result['retrieved_count'],
                'query_language': 'zh' if any('\u4e00' <= char <= '\u9fff' for char in question) else 'en',
                'answer_language': answer_language,
                'used_rag': True,
                'timing': rag_result['timing']
            })
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæœç´¢
            search_results = search_in_questions(
                question, 
                questions_data, 
                answer_language=answer_language,
                top_k=5
            )
            
            answer_html = generate_answer_html(question, search_results, answer_language)
            
            result_count = len(search_results)
            if search_results:
                avg_confidence = sum(r.get('confidence', 0.5) for r in search_results) / result_count
            else:
                avg_confidence = 0
            
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in question)
            query_language = 'zh' if has_chinese else 'en'
            
            return jsonify({
                'success': True,
                'question': question,
                'answer': answer_html,
                'confidence': avg_confidence,
                'result_count': result_count,
                'query_language': query_language,
                'answer_language': answer_language,
                'used_rag': False
            })
    
    except Exception as e:
        print(f"æŸ¥è¯¢å¤„ç†é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        })

def generate_answer_html(question, search_results, answer_language='zh'):
    """ç”Ÿæˆä¼ ç»Ÿæœç´¢çš„å›ç­”HTML"""
    if not search_results:
        return f'''
        <div class="no-results">
            <h4>ğŸ¤” æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯</h4>
            <p>æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ä¸"<strong>{question}</strong>"ç›´æ¥ç›¸å…³çš„åŒ»ç–—ä¿¡æ¯ã€‚</p>
            <div class="suggestions">
                <p>å»ºè®®ï¼š</p>
                <ul>
                    <li>å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„åŒ»ç–—æœ¯è¯­ï¼ˆå¦‚"ç³–å°¿ç—…ç—‡çŠ¶"ã€"é«˜è¡€å‹æ²»ç–—"ï¼‰</li>
                    <li>æ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«æ‹¼å†™é”™è¯¯</li>
                    <li>å°è¯•è¯¢é—®å¸¸è§ç–¾ç—…ï¼ˆå¦‚æ„Ÿå†’ã€å¤´ç—›ã€ç³–å°¿ç—…ç­‰ï¼‰</li>
                    <li>æ‚¨ä¹Ÿå¯ä»¥ç”¨è‹±æ–‡æé—®</li>
                </ul>
            </div>
        </div>
        '''
    
    html_parts = []
    
    html_parts.append('<div class="answer-container">')
    html_parts.append('<h4>ğŸ” æŸ¥è¯¢ç»“æœï¼ˆä¼ ç»Ÿæœç´¢ï¼‰</h4>')
    html_parts.append(f'<p class="query-display">é—®é¢˜ï¼š<strong>{question}</strong></p>')
    
    for i, result in enumerate(search_results, 1):
        display_question = result.get('display_question', '')
        display_answer = result.get('display_answer', '')
        source = result.get('source', 'åŒ»ç–—æ•°æ®åº“')
        q_type = result.get('type', 'åŒ»ç–—ä¿¡æ¯')
        confidence = result.get('confidence', 0.7) * 100
        
        html_parts.append(f'''
        <div class="search-result">
            <div class="result-header">
                <span class="result-number">#{i}</span>
                <span class="result-type">{q_type}</span>
                <span class="result-confidence">ç½®ä¿¡åº¦: {confidence:.0f}%</span>
            </div>
            <div class="result-content">
                <p><strong>ç›¸å…³ä¿¡æ¯:</strong> {display_question}</p>
                <div class="answer-box">
                    <strong>ç­”æ¡ˆ:</strong> {display_answer}
                </div>
                <p style="margin-top: 10px;"><strong>æ¥æº:</strong> <span class="source-badge">{source}</span></p>
            </div>
        </div>
        ''')
    
    advice = '''
    <div class="medical-advice">
        <h5>ğŸ’¡ åŒ»ç–—å»ºè®®</h5>
        <ul>
            <li>ä»¥ä¸Šä¿¡æ¯åŸºäºåŒ»ç–—æ•°æ®åº“ï¼Œä»…ä¾›å‚è€ƒ</li>
            <li>å…·ä½“ç—‡çŠ¶è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ</li>
            <li>å¦‚é‡ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å°±åŒ»</li>
            <li>ä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼æœ‰åŠ©äºç–¾ç—…é¢„é˜²</li>
        </ul>
    </div>
    '''
    
    html_parts.append(advice)
    html_parts.append('</div>')
    return '\n'.join(html_parts)

def generate_rag_answer_html(question, rag_result, answer_language='zh'):
    """ç”ŸæˆRAGå›ç­”çš„HTML"""
    answer = rag_result.get('answer', '')
    confidence = rag_result.get('confidence', 0.5) * 100
    source_documents = rag_result.get('source_documents', [])
    timing = rag_result.get('timing', {})
    
    html_parts = []
    
    html_parts.append('<div class="answer-container rag-answer">')
    html_parts.append('<h4>ğŸ§  æ™ºèƒ½åˆ†æç»“æœï¼ˆRAGç³»ç»Ÿï¼‰</h4>')
    html_parts.append(f'<p class="query-display">é—®é¢˜ï¼š<strong>{question}</strong></p>')
    
    # æ˜¾ç¤ºRAGç³»ç»Ÿä¿¡æ¯
    html_parts.append(f'''
    <div class="rag-info">
        <div class="rag-metrics">
            <span class="rag-metric"><strong>ç½®ä¿¡åº¦:</strong> {confidence:.0f}%</span>
            <span class="rag-metric"><strong>æ£€ç´¢æ–‡æ¡£:</strong> {len(source_documents)} ä¸ª</span>
            <span class="rag-metric"><strong>æ£€ç´¢æ—¶é—´:</strong> {timing.get('retrieval', 'N/A')}</span>
            <span class="rag-metric"><stron.g>ç”Ÿæˆæ—¶é—´:</strong> {timing.get('generation', 'N/A')}</span>
        </div>
    </div>
    ''')
    
    # æ˜¾ç¤ºç”Ÿæˆçš„ç­”æ¡ˆ
    answer_html = answer.replace('\n', '<br>')
    html_parts.append(f'''
    <div class="generated-answer">
        <h5>ğŸ’¬ ç”Ÿæˆç­”æ¡ˆï¼š</h5>
        <div class="answer-content">
            {answer_html}
        </div>
    </div>
    ''')
    
    # æ˜¾ç¤ºæºæ–‡æ¡£
    if source_documents:
        html_parts.append('''
        <div class="source-documents">
            <h5>ğŸ“š å‚è€ƒæ¥æºï¼š</h5>
        ''')
        
        for i, source in enumerate(source_documents, 1):
            source_type_badge = {
                'semantic_search': 'ğŸ” è¯­ä¹‰åŒ¹é…',
                'keyword_search': 'ğŸ”‘ å…³é”®è¯åŒ¹é…',
                'question_search': 'â“ é—®é¢˜åº“åŒ¹é…'
            }.get(source.get('source_type', 'unknown'), 'ğŸ“„ æ–‡æ¡£')
            
            html_parts.append(f'''
            <div class="source-document">
                <div class="source-header">
                    <span class="source-number">#{i}</span>
                    <span class="source-type">{source_type_badge}</span>
                    <span class="source-confidence">ç›¸å…³åº¦: {source.get('confidence', 0.5)*100:.0f}%</span>
                </div>
                <div class="source-content">
                    {source.get('content', '')}
                </div>
            </div>
            ''')
        
        html_parts.append('</div>')
    
    # åŒ»ç–—å»ºè®®
    html_parts.append('''
    <div class="medical-advice">
        <h5>ğŸ’¡ åŒ»ç–—å»ºè®®</h5>
        <ul>
            <li>ä»¥ä¸Šä¿¡æ¯åŸºäºåŒ»ç–—çŸ¥è¯†åº“çš„æ™ºèƒ½åˆ†æï¼Œä»…ä¾›å‚è€ƒ</li>
            <li>å…·ä½“ç—‡çŠ¶è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ</li>
            <li>å¦‚é‡ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å°±åŒ»</li>
            <li>ä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼æœ‰åŠ©äºç–¾ç—…é¢„é˜²</li>
        </ul>
    </div>
    ''')
    
    html_parts.append('</div>')
    return '\n'.join(html_parts)

@app.route('/api/export-data')
def export_data():
    """å¯¼å‡ºæ•°æ®ä¸ºExcel"""
    try:
        corpus_data = load_corpus_data()
        questions_data = load_questions_data()
        
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp:
            with pd.ExcelWriter(tmp.name, engine='openpyxl') as writer:
                if corpus_data and 'paragraphs' in corpus_data:
                    corpus_df = pd.DataFrame({
                        'Paragraph ID': [f'P{i+1:03d}' for i in range(len(corpus_data['paragraphs']))],
                        'Content': corpus_data['paragraphs'][:100],
                        'Character Count': [len(p) for p in corpus_data['paragraphs'][:100]]
                    })
                    corpus_df.to_excel(writer, sheet_name='Corpus Content', index=False)
                
                if questions_data and 'sample_questions' in questions_data:
                    questions_list = []
                    for i, q in enumerate(questions_data['sample_questions'][:200]):
                        questions_list.append({
                            'No.': i+1,
                            'Question (CN)': q.get('question_cn', ''),
                            'Question (EN)': q.get('question_en', ''),
                            'Answer (CN)': q.get('answer_cn', '')[:200] if q.get('answer_cn') else '',
                            'Answer (EN)': q.get('answer_en', '')[:200] if q.get('answer_en') else '',
                            'Type': q.get('type', 'Unknown'),
                            'Source': q.get('source', 'Unknown')
                        })
                    
                    questions_df = pd.DataFrame(questions_list)
                    questions_df.to_excel(writer, sheet_name='Question Samples', index=False)
                
                stats_data = [
                    {'Item': 'Corpus', 'Value': corpus_data['doc_count'] if corpus_data else 1, 'Description': 'Number of documents'},
                    {'Item': 'Question Set', 'Value': questions_data['total_count'] if questions_data else 0, 'Description': 'Total questions'},
                ]
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='Statistics', index=False)
            
            tmp_path = tmp.name
        
        return send_file(
            tmp_path,
            as_attachment=True,
            download_name='Medical_RAG_System_Data.xlsx',
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/data-stats')
def data_stats():
    """è·å–æ•°æ®ç»Ÿè®¡API"""
    doc_count, question_count, corpus_data, questions_data = get_data_counts()
    
    stats = {
        'corpus': {
            'document_count': doc_count,
            'corpus_name': corpus_data.get('corpus_name', 'åŒ»ç–—çŸ¥è¯†åº“') if corpus_data else 'Unknown',
            'has_data': corpus_data is not None
        },
        'questions': {
            'total_count': question_count,
            'type_count': len(questions_data.get('question_types', {})) if questions_data else 0,
            'has_data': questions_data is not None
        },
        'rag': {
            'enabled': HAS_EMBEDDING,
            'chunk_size': RAG_CONFIG['chunk_size'],
            'top_k_retrieval': RAG_CONFIG['top_k_retrieval'],
            'hybrid_search': RAG_CONFIG['hybrid_search']
        }
    }
    
    return jsonify({'success': True, 'data': stats})

@app.route('/api/rag-status')
def rag_status():
    """è·å–RAGç³»ç»ŸçŠ¶æ€"""
    return jsonify({
        'success': True,
        'rag_enabled': HAS_EMBEDDING,
        'vector_store_ready': vector_store is not None and len(vector_store.get('corpus_chunks', [])) > 0,
        'config': RAG_CONFIG
    })

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ§  åŒè¯­åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿ (RAGå¢å¼ºç‰ˆ)")
    print("=" * 60)
    print("ğŸ“‚ æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    if CORPUS_PATH.exists():
        print(f"   âœ“ è¯­æ–™åº“æ–‡ä»¶: {CORPUS_PATH}")
    else:
        print(f"   âœ— è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {CORPUS_PATH}")
        print(f"     è¯·å°† medical_corpus.json æ”¾ç½®åœ¨: {CORPUS_PATH}")
    if QUESTIONS_PATH.exists():
        print(f"   âœ“ é—®é¢˜é›†æ–‡ä»¶: {QUESTIONS_PATH}")
    else:
        print(f"   âœ— é—®é¢˜é›†æ–‡ä»¶ä¸å­˜åœ¨: {QUESTIONS_PATH}")
        print(f"     è¯·å°† medical_questions.json æ”¾ç½®åœ¨: {QUESTIONS_PATH}")
    # å¯åŠ¨æ—¶å…¨å±€åŠ è½½æ•°æ®å’Œå‘é‡
    initialize_data_and_vectors()
    doc_count, question_count, _, _ = get_data_counts()
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   è¯­æ–™åº“: {doc_count} ç¯‡æ–‡æ¡£")
    print(f"   é—®é¢˜é›†: {question_count} ä¸ªé—®é¢˜")
    if GLOBAL_VECTOR_STORE_READY:
        print(f"\nğŸ”§ RAGç³»ç»Ÿå·²å°±ç»ª")
    else:
        print(f"\nâš ï¸  RAGç³»ç»Ÿæœªå¯ç”¨æˆ–æ•°æ®ä¸å®Œæ•´")
        if not HAS_EMBEDDING:
            print(f"   è¯·å®‰è£…: pip install sentence-transformers")
    print(f"\nğŸŒ è®¿é—®åœ°å€: http://localhost:5000")
    print(f"\nâš¡ ç³»ç»Ÿç‰¹æ€§:")
    print(f"   â€¢ æ”¯æŒä¸­è‹±æ–‡ä»»æ„è¯­è¨€æé—®")
    print(f"   â€¢ å¯é€‰æ‹©ä¸­æ–‡æˆ–è‹±æ–‡å›ç­”")
    print(f"   â€¢ RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ")
    print(f"   â€¢ æ··åˆæœç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰")
    print(f"   â€¢ æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆ")
    print(f"   â€¢ æ•°æ®å¯¼å‡ºåŠŸèƒ½")
    print("\nğŸ¯ ä½¿ç”¨è¯´æ˜:")
    print(f"   1. åœ¨è¾“å…¥æ¡†ä¸­ç”¨ä¸­æ–‡æˆ–è‹±æ–‡æé—®")
    print(f"   2. é€‰æ‹©æƒ³è¦çš„å›ç­”è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰")
    print(f"   3. ç³»ç»Ÿä¼šä½¿ç”¨RAGæ™ºèƒ½æ£€ç´¢ç›¸å…³ä¿¡æ¯")
    print(f"   4. å¯ä»¥ç‚¹å‡»'ç¤ºä¾‹é—®é¢˜'å¿«é€Ÿæµ‹è¯•")
    print(f"   5. å¯åœ¨å‰ç«¯é€‰æ‹©å¯ç”¨/ç¦ç”¨RAGåŠŸèƒ½")
    print("=" * 60)
    time.sleep(1)
    os.makedirs('templates', exist_ok=True)
    app.run(debug=True, host='0.0.0.0', port=5000)