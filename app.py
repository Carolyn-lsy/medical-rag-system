# app.py - åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿ
import os
import sys
import traceback
import streamlit as st
import json  # æ–°å¢å¯¼å…¥
from pathlib import Path  # æ–°å¢å¯¼å…¥

# ========== ç¬¬ä¸€æ­¥ï¼šä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜ ==========
# å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰æ‰§è¡Œï¼
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

print("=== ç³»ç»Ÿå¯åŠ¨ ===")
print(f"é¡¹ç›®ç›®å½•: {current_dir}")
print(f"å½“å‰Pythonè·¯å¾„å‰3é¡¹: {sys.path[:3]}")

# ========== æ–°å¢å‡½æ•°ï¼šåŠ¨æ€è·å–æ•°æ®æ•°é‡ ==========
def get_actual_data_counts():
    """
    åŠ¨æ€è·å–å®é™…JSONæ–‡ä»¶ä¸­çš„æ•°æ®æ•°é‡
    è¿”å›: (å®é™…æ–‡æ¡£æ•°, å®é™…é—®é¢˜æ•°)
    """
    try:
        # 1. è·å–å®é™…è¯­æ–™åº“æ•°é‡
        corpus_path = Path("data/raw/medical_corpus.json")
        actual_doc_count = 0
        if corpus_path.exists():
            with open(corpus_path, 'r', encoding='utf-8') as f:
                corpus = json.load(f)
                if isinstance(corpus, dict):
                    # æ ¹æ®æ•°æ®ç»“æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œcontextæ˜¯ä¸€ä¸ªé•¿å­—ç¬¦ä¸²
                    if 'context' in corpus:
                        actual_doc_count = 1
                    else:
                        actual_doc_count = len(corpus)
                elif isinstance(corpus, list):
                    actual_doc_count = len(corpus)
                else:
                    actual_doc_count = 1
            print(f"âœ“ å®é™…è¯­æ–™åº“æ–‡æ¡£æ•°: {actual_doc_count}")
        else:
            print(f"âš ï¸ è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_path}")
        
        # 2. è·å–å®é™…é—®é¢˜é›†æ•°é‡
        questions_path = Path("data/raw/medical_questions.json")
        actual_question_count = 0
        if questions_path.exists():
            with open(questions_path, 'r', encoding='utf-8') as f:
                questions = json.load(f)
                if isinstance(questions, list):
                    actual_question_count = len(questions)
                elif isinstance(questions, dict):
                    if 'questions' in questions:
                        actual_question_count = len(questions['questions'])
                    else:
                        actual_question_count = len(questions)
                else:
                    actual_question_count = 1
            print(f"âœ“ å®é™…é—®é¢˜é›†é—®é¢˜æ•°: {actual_question_count}")
        else:
            print(f"âš ï¸ é—®é¢˜é›†æ–‡ä»¶ä¸å­˜åœ¨: {questions_path}")
        
        return actual_doc_count, actual_question_count
        
    except Exception as e:
        print(f"âŒ è·å–å®é™…æ•°æ®æ•°é‡å¤±è´¥: {e}")
        return 0, 0

# åœ¨Streamlitæ¸²æŸ“ä¹‹å‰è·å–å®é™…æ•°æ®
actual_doc_count, actual_question_count = get_actual_data_counts()

# ========== ç¬¬äºŒæ­¥ï¼šå°è¯•å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ==========
print("\n=== æ¨¡å—å¯¼å…¥æµ‹è¯• ===")

try:
    # å…ˆå°è¯•ç›´æ¥å¯¼å…¥
    from src.config import Config
    from src.data_loader import MedicalDataLoader  # æ³¨æ„ï¼šç±»åæ˜¯ MedicalDataLoader
    from src.preprocessor import TextPreprocessor  # æ³¨æ„ï¼šç±»åæ˜¯ TextPreprocessor
    from src.vector_store import VectorStore
    from src.answer_generator import AnswerGenerator
    
    print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    IMPORT_METHOD = "success"
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    traceback.print_exc()
    
    # å¦‚æœå¤±è´¥ï¼Œåˆ›å»ºè™šæ‹Ÿç±»
    print("âš ï¸  ä½¿ç”¨è™šæ‹Ÿæ¨¡å—ä½œä¸ºåå¤‡")
    
    class Config:
        MILVUS_HOST = 'localhost'
        MILVUS_PORT = '19530'
        EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        EMBEDDING_DIM = 384
        CHUNK_SIZE = 500
        CHUNK_OVERLAP = 50
        TOP_K = 3
    
    class MedicalDataLoader:
        def __init__(self):
            print("è™šæ‹Ÿ: MedicalDataLoaderåˆå§‹åŒ–")
        def load_from_json(self, filepath):
            print(f"è™šæ‹Ÿ: ä» {filepath} åŠ è½½æ•°æ®")
            return []
        def load_medical_corpus(self, corpus_path="data/medical.json"):
            return self.load_from_json(corpus_path)
        def load_medical_questions(self, questions_path="data/medical_questions.json"):
            return self.load_from_json(questions_path)
    
    class TextPreprocessor:
        def __init__(self):
            print("è™šæ‹Ÿ: TextPreprocessoråˆå§‹åŒ–")
        def clean_html(self, html_text):
            return html_text
        def split_into_chunks(self, text, chunk_size=500, overlap=50):
            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]
        def process_document(self, document):
            text = document.get('content', '') or document.get('text', '')
            chunks = self.split_into_chunks(text)
            return [{'text': chunk, 'chunk_id': i} for i, chunk in enumerate(chunks)]
    
    class VectorStore:
        def __init__(self, collection_name="medical_docs"):
            print(f"è™šæ‹Ÿ: VectorStoreåˆå§‹åŒ–ï¼Œé›†åˆå: {collection_name}")
        def create_collection(self):
            print("è™šæ‹Ÿ: åˆ›å»ºé›†åˆ")
        def insert_documents(self, documents, embeddings):
            print(f"è™šæ‹Ÿ: æ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£")
        def search(self, query_embedding, top_k=3):
            print(f"è™šæ‹Ÿ: æœç´¢ï¼Œtop_k={top_k}")
            return []
    
    class AnswerGenerator:
        def __init__(self):
            print("è™šæ‹Ÿ: AnswerGeneratoråˆå§‹åŒ–")
        def generate_answer(self, question, search_results):
            return f"è¿™æ˜¯å…³äº '{question}' çš„è™šæ‹Ÿå›ç­”ã€‚å®é™…ç³»ç»Ÿä¸­ä¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆä¸“ä¸šå›ç­”ã€‚"
    
    IMPORT_METHOD = "virtual"

# ========== ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–æ¨¡å—å®ä¾‹ ==========
print(f"\n=== æ¨¡å—åˆå§‹åŒ– (æ–¹å¼: {IMPORT_METHOD}) ===")

try:
    if IMPORT_METHOD == "success":
        config = Config()
        loader = MedicalDataLoader()      # ä½¿ç”¨æ­£ç¡®çš„ç±»å
        processor = TextPreprocessor()    # ä½¿ç”¨æ­£ç¡®çš„ç±»å
        vector_store = VectorStore()
        answer_generator = AnswerGenerator()
        print(f"âœ… ä½¿ç”¨å®é™…æ¨¡å—åˆå§‹åŒ–")
    else:
        config = Config()
        loader = MedicalDataLoader()
        processor = TextPreprocessor()
        vector_store = VectorStore()
        answer_generator = AnswerGenerator()
        print(f"âš ï¸  ä½¿ç”¨è™šæ‹Ÿæ¨¡å—åˆå§‹åŒ–")
    
    print(f"æ•°æ®åŠ è½½å™¨: {type(loader).__name__}")
    print(f"æ–‡æœ¬å¤„ç†å™¨: {type(processor).__name__}")
    
except Exception as e:
    print(f"âŒ æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
    traceback.print_exc()

# ========== ç¬¬å››æ­¥ï¼šæ•°æ®åŠ è½½ ==========
print("\n=== æ•°æ®åŠ è½½ ===")

# åˆ›å»ºç¤ºä¾‹æ•°æ® - ä¿æŒä¸å˜
corpus_data = [
    {
        "id": "doc_001",
        "title": "ç³–å°¿ç—…åŸºç¡€çŸ¥è¯†",
        "content": "ç³–å°¿ç—…æ˜¯ä¸€ç§æ…¢æ€§ç–¾ç—…ï¼Œä¸»è¦ç‰¹å¾æ˜¯è¡€ç³–æ°´å¹³æŒç»­å‡é«˜ã€‚å¸¸è§ç—‡çŠ¶åŒ…æ‹¬å¤šé¥®ã€å¤šå°¿ã€å¤šé£Ÿã€ä½“é‡ä¸‹é™ç­‰ã€‚é¢„é˜²æ–¹æ³•åŒ…æ‹¬å¥åº·é¥®é£Ÿã€å®šæœŸè¿åŠ¨å’Œä¿æŒå¥åº·ä½“é‡ã€‚",
        "source": "åŒ»ç–—æŒ‡å—"
    },
    {
        "id": "doc_002", 
        "title": "é«˜è¡€å‹é˜²æ²»",
        "content": "é«˜è¡€å‹æ˜¯æŒ‡è¡€å‹æŒç»­åé«˜çš„çŠ¶æ€ã€‚é¢„é˜²æ–¹æ³•åŒ…æ‹¬ä½ç›é¥®é£Ÿã€å®šæœŸé”»ç‚¼ã€æ§åˆ¶ä½“é‡ã€æˆ’çƒŸé™é…’ã€‚ç—‡çŠ¶å¯èƒ½åŒ…æ‹¬å¤´ç—›ã€å¤´æ™•ã€å¿ƒæ‚¸ç­‰ï¼Œä½†å¾ˆå¤šæ‚£è€…æ— ç—‡çŠ¶ã€‚",
        "source": "å¿ƒè¡€ç®¡å¥åº·æ‰‹å†Œ"
    },
    {
        "id": "doc_003",
        "title": "æ„Ÿå†’ç—‡çŠ¶ä¸æ²»ç–—",
        "content": "æ™®é€šæ„Ÿå†’æ˜¯ç”±ç—…æ¯’å¼•èµ·çš„ä¸Šå‘¼å¸é“æ„ŸæŸ“ã€‚å¸¸è§ç—‡çŠ¶åŒ…æ‹¬æµé¼»æ¶•ã€å’³å—½ã€å–‰å’™ç—›ã€æ‰“å–·åšã€è½»å¾®å‘çƒ­ç­‰ã€‚å»ºè®®å¤šä¼‘æ¯ã€å¤šå–æ°´ï¼Œå¦‚æœ‰éœ€è¦å¯æœç”¨éå¤„æ–¹è¯ç‰©ç¼“è§£ç—‡çŠ¶ã€‚",
        "source": "å®¶åº­åŒ»å­¦æŒ‡å—"
    }
]

questions_data = [
    {"question": "ç³–å°¿ç—…çš„å¸¸è§ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ", "answer": "å¤šé¥®ã€å¤šå°¿ã€å¤šé£Ÿã€ä½“é‡ä¸‹é™ç­‰"},
    {"question": "å¦‚ä½•é¢„é˜²é«˜è¡€å‹ï¼Ÿ", "answer": "ä½ç›é¥®é£Ÿã€å®šæœŸé”»ç‚¼ã€æ§åˆ¶ä½“é‡ã€æˆ’çƒŸé™é…’"},
    {"question": "æ„Ÿå†’æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ", "answer": "æµé¼»æ¶•ã€å’³å—½ã€å–‰å’™ç—›ã€æ‰“å–·åšã€è½»å¾®å‘çƒ­"}
]

print(f"ç¤ºä¾‹è¯­æ–™åº“æ–‡æ¡£æ•°: {len(corpus_data)}")
print(f"ç¤ºä¾‹æµ‹è¯•é—®é¢˜æ•°: {len(questions_data)}")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'vector_store_initialized' not in st.session_state:
    st.session_state.vector_store_initialized = False

# Streamlité¡µé¢é…ç½®
st.set_page_config(
    page_title="åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========== ä¾§è¾¹æ  ==========
with st.sidebar:
    st.title("ğŸ¥ åŒ»ç–—RAGç³»ç»Ÿ")
    st.markdown("---")
    
    # ç³»ç»ŸçŠ¶æ€ - ä¿®æ”¹ï¼šæ˜¾ç¤ºå®é™…æ•°æ®æ•°é‡
    st.header("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    
    # ä½¿ç”¨å®é™…æ•°æ®æ•°é‡
    display_doc_count = actual_doc_count if actual_doc_count > 0 else len(corpus_data)
    display_question_count = actual_question_count if actual_question_count > 0 else len(questions_data)
    
    st.info(f"ğŸ“ è¯­æ–™åº“: {display_doc_count} ç¯‡æ–‡æ¡£")
    st.info(f"â“ é—®é¢˜é›†: {display_question_count} ä¸ªé—®é¢˜")
    st.info(f"ğŸ”§ å¯¼å…¥æ–¹å¼: {IMPORT_METHOD}")
    
    # æ·»åŠ å®é™…æ•°æ®æºä¿¡æ¯
    if actual_doc_count > 0 or actual_question_count > 0:
        with st.expander("ğŸ“‚ å®é™…æ•°æ®ä¿¡æ¯"):
            st.write(f"**å®é™…è¯­æ–™åº“**: {actual_doc_count}ç¯‡æ–‡æ¡£")
            st.write(f"**å®é™…é—®é¢˜é›†**: {actual_question_count}ä¸ªé—®é¢˜")
            if actual_doc_count == 0 and actual_question_count == 0:
                st.write("âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            else:
                st.write("âœ… ä»JSONæ–‡ä»¶åŠ è½½å®é™…æ•°æ®")
    
    st.markdown("---")
    
    # ç³»ç»Ÿè®¾ç½® - ä¿æŒä¸å˜
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    top_k = st.slider(
        "æ£€ç´¢ç»“æœæ•°é‡", 
        min_value=1, 
        max_value=10, 
        value=3,
        help="æ¯æ¬¡æŸ¥è¯¢è¿”å›çš„æœ€ç›¸å…³æ–‡æ¡£æ•°é‡"
    )
    
    chunk_size = st.slider(
        "æ–‡æœ¬åˆ†å—å¤§å°", 
        min_value=100, 
        max_value=1000, 
        value=500,
        step=50,
        help="æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"
    )
    
    similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é˜ˆå€¼", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.6,
        step=0.05,
        help="åªæ˜¾ç¤ºç›¸ä¼¼åº¦é«˜äºæ­¤å€¼çš„ç»“æœ"
    )
    
    st.markdown("---")
    st.header("ğŸ“ æ•°æ®æ“ä½œ")
    
    if st.button("ğŸ§ª æµ‹è¯•æ–‡æœ¬å¤„ç†", use_container_width=True):
        if corpus_data and len(corpus_data) > 0:
            sample_text = corpus_data[0].get('content', '')
            if sample_text:
                chunks = processor.split_into_chunks(sample_text, chunk_size, 50)
                st.success(f"æµ‹è¯•æˆåŠŸï¼å°†æ–‡æœ¬åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
                with st.expander("æŸ¥çœ‹åˆ†å—è¯¦æƒ…"):
                    for i, chunk in enumerate(chunks[:3]):
                        st.text(f"å— {i+1}: {chunk[:80]}...")
            else:
                st.warning("æ ·æœ¬æ–‡æœ¬ä¸ºç©º")
        else:
            st.warning("è¯­æ–™åº“ä¸ºç©º")
    
    # æ·»åŠ æ•°æ®é‡æ–°åŠ è½½æŒ‰é’®
    if st.button("ğŸ”„ é‡æ–°åŠ è½½æ•°æ®ç»Ÿè®¡", use_container_width=True):
        new_doc_count, new_question_count = get_actual_data_counts()
        st.success(f"æ•°æ®ç»Ÿè®¡å·²æ›´æ–°: {new_doc_count}ç¯‡æ–‡æ¡£, {new_question_count}ä¸ªé—®é¢˜")
        # å¯ä»¥ä½¿ç”¨session_stateä¿å­˜æ–°å€¼
    
    st.markdown("---")
    st.caption("åŸºäºå‘é‡æ£€ç´¢çš„åŒ»ç–—é—®ç­”ç³»ç»Ÿ v1.0")

# ========== ä¸»ç•Œé¢ ==========
st.title("ğŸ” åŒ»ç–—é—®é¢˜æŸ¥è¯¢ç³»ç»Ÿ")
st.markdown("""
æ¬¢è¿ä½¿ç”¨åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿï¼æœ¬ç³»ç»ŸåŸºäºåŒ»ç–—çŸ¥è¯†åº“ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä¸ºæ‚¨æä¾›å‡†ç¡®çš„åŒ»ç–—ä¿¡æ¯å›ç­”ã€‚
**æ³¨æ„**ï¼šæœ¬ç³»ç»Ÿæä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚
""")

# æ˜¾ç¤ºå®é™…æ•°æ®ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if actual_doc_count > 0 or actual_question_count > 0:
    st.info(f"ğŸ“Š **å®é™…æ•°æ®ç»Ÿè®¡**: è¯­æ–™åº“ {actual_doc_count}ç¯‡æ–‡æ¡£ | é—®é¢˜é›† {actual_question_count}ä¸ªé—®é¢˜")

# å¿«é€Ÿé—®é¢˜ç¤ºä¾‹ - ä¿æŒä¸å˜
st.subheader("ğŸ’¡ è¯•è¯•è¿™äº›é—®é¢˜:")
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("ç³–å°¿ç—…çš„ç—‡çŠ¶", use_container_width=True, type="secondary"):
        st.session_state.auto_question = "ç³–å°¿ç—…çš„å¸¸è§ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ"
with col2:
    if st.button("é«˜è¡€å‹é¢„é˜²", use_container_width=True, type="secondary"):
        st.session_state.auto_question = "å¦‚ä½•é¢„é˜²é«˜è¡€å‹ï¼Ÿ"
with col3:
    if st.button("æ„Ÿå†’çš„ç—‡çŠ¶", use_container_width=True, type="secondary"):
        st.session_state.auto_question = "æ„Ÿå†’æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ"

# æŸ¥è¯¢åŒºåŸŸ - ä¿æŒä¸å˜
st.subheader("ğŸ” é—®é¢˜æŸ¥è¯¢")
query_col1, query_col2 = st.columns([4, 1])

with query_col1:
    # å¦‚æœæœ‰è‡ªåŠ¨å¡«å……çš„é—®é¢˜ï¼Œä½¿ç”¨å®ƒ
    default_question = ""
    if 'auto_question' in st.session_state:
        default_question = st.session_state.auto_question
        # ä½¿ç”¨åæ¸…é™¤
        del st.session_state.auto_question
    
    question = st.text_input(
        "è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜ï¼š",
        value=default_question,
        placeholder="ä¾‹å¦‚ï¼šç³–å°¿ç—…çš„å¸¸è§ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿå¦‚ä½•é¢„é˜²é«˜è¡€å‹ï¼Ÿ",
        label_visibility="collapsed",
        key="question_input"
    )

with query_col2:
    search_button = st.button("ğŸ” å¼€å§‹æŸ¥è¯¢", 
                            type="primary", 
                            use_container_width=True)

# ========== æŸ¥è¯¢å¤„ç† ==========
if search_button and question:
    with st.spinner("æ­£åœ¨æ£€ç´¢å’Œç”Ÿæˆå›ç­”..."):
        try:
            # è®°å½•æŸ¥è¯¢
            st.session_state.search_history.append({
                'question': question,
                'time': len(st.session_state.search_history) + 1
            })
            
            # æ˜¾ç¤ºæŸ¥è¯¢é—®é¢˜
            st.success(f"ğŸ“ æŸ¥è¯¢é—®é¢˜: **{question}**")
            
            # === æ­¥éª¤1: æ–‡æœ¬é¢„å¤„ç† ===
            st.subheader("ğŸ“‹ ç¬¬ä¸€æ­¥: æ–‡æœ¬å¤„ç†")
            
            # ç®€å•çš„æ–‡æœ¬å¤„ç†æ¼”ç¤º
            if hasattr(processor, 'split_into_chunks'):
                chunks = processor.split_into_chunks(question, 100, 20)
                with st.expander("æŸ¥çœ‹æŸ¥è¯¢æ–‡æœ¬åˆ†å—", expanded=False):
                    st.write(f"åŸå§‹é—®é¢˜: {question}")
                    st.write(f"åˆ†å—æ•°é‡: {len(chunks)}")
                    for i, chunk in enumerate(chunks):
                        st.text(f"å— {i+1}: {chunk}")
            
            # === æ­¥éª¤2: æ£€ç´¢ç›¸å…³æ–‡æ¡£ ===
            st.subheader("ğŸ” ç¬¬äºŒæ­¥: æ£€ç´¢ç›¸å…³æ–‡æ¡£")
            
            # ç®€å•å…³é”®è¯åŒ¹é…
            keywords = []
            if "ç³–å°¿ç—…" in question:
                keywords = ["ç³–å°¿ç—…", "è¡€ç³–", "èƒ°å²›ç´ "]
            elif "é«˜è¡€å‹" in question:
                keywords = ["é«˜è¡€å‹", "è¡€å‹", "å¿ƒè¡€ç®¡"]
            elif "æ„Ÿå†’" in question:
                keywords = ["æ„Ÿå†’", "ç—…æ¯’", "å‘¼å¸é“"]
            else:
                # é€šç”¨å…³é”®è¯æå–
                import re
                keywords = re.findall(r'[\u4e00-\u9fff]+', question)
            
            relevant_docs = []
            for doc in corpus_data:
                content = doc.get('content', '')
                title = doc.get('title', '')
                full_text = title + " " + content
                
                match_score = 0
                for kw in keywords:
                    if kw in full_text:
                        match_score += 1
                
                if match_score > 0:
                    relevant_docs.append({
                        'doc': doc,
                        'score': match_score,
                        'preview': content[:150] + '...' if len(content) > 150 else content
                    })
            
            # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
            relevant_docs.sort(key=lambda x: x['score'], reverse=True)
            relevant_docs = relevant_docs[:top_k]
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            if relevant_docs:
                st.success(f"æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                
                for i, result in enumerate(relevant_docs, 1):
                    doc = result['doc']
                    with st.expander(f"ğŸ“„ ç»“æœ {i}: {doc.get('title', 'æ— æ ‡é¢˜')} (åŒ¹é…åº¦: {result['score']})", expanded=True):
                        st.write(f"**æ¥æº**: {doc.get('source', 'æœªçŸ¥æ¥æº')}")
                        st.write(f"**å†…å®¹**: {result['preview']}")
                        
                        # æ˜¾ç¤ºæ–‡æœ¬åˆ†å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if hasattr(processor, 'process_document'):
                            processed = processor.process_document(doc)
                            if processed:
                                st.write(f"**æ–‡æœ¬åˆ†å—**: å…± {len(processed)} å—")
                                for chunk in processed[:2]:
                                    st.text(f"â€¢ {chunk.get('text', '')[:100]}...")
            else:
                st.warning("æœªæ‰¾åˆ°é«˜åº¦ç›¸å…³æ–‡æ¡£ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†å›ç­”")
                relevant_docs = [{
                    'doc': {'title': 'é€šç”¨åŒ»ç–—çŸ¥è¯†', 'content': 'åŸºäºå¸¸è§åŒ»ç–—çŸ¥è¯†æä¾›å›ç­”ã€‚'},
                    'score': 0,
                    'preview': 'é€šç”¨åŒ»ç–—çŸ¥è¯†åº“'
                }]
            
            # === æ­¥éª¤3: ç”Ÿæˆå›ç­” ===
            st.subheader("ğŸ’¡ ç¬¬ä¸‰æ­¥: ç³»ç»Ÿå›ç­”")
            
            # ä½¿ç”¨AnswerGeneratorç”Ÿæˆå›ç­”
            if IMPORT_METHOD == "success":
                try:
                    # å‡†å¤‡æœç´¢ç»“æœæ ¼å¼
                    search_results = []
                    for result in relevant_docs:
                        search_results.append({
                            'text': result['preview'],
                            'metadata': {
                                'title': result['doc'].get('title', ''),
                                'source': result['doc'].get('source', '')
                            }
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    answer = answer_generator.generate_answer(question, search_results)
                    
                except Exception as e:
                    st.warning(f"AnswerGeneratorç”Ÿæˆå¤±è´¥: {e}")
                    answer = f"""
                    åŸºäºæ£€ç´¢åˆ°çš„åŒ»ç–—çŸ¥è¯†ï¼Œå…³äº"**{question}**"ï¼š
                    
                    **ç›¸å…³ä¿¡æ¯æ€»ç»“**:
                    {chr(10).join([f"{i+1}. {r['preview']}" for i, r in enumerate(relevant_docs)])}
                    
                    **å»ºè®®**:
                    - ä»¥ä¸Šä¿¡æ¯åŸºäºåŒ»ç–—çŸ¥è¯†åº“
                    - å…·ä½“ç—‡çŠ¶è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
                    - ä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼
                    """
            else:
                # ä½¿ç”¨è™šæ‹Ÿå›ç­”
                answer = answer_generator.generate_answer(question, [])
            
            # æ˜¾ç¤ºå›ç­”
            st.markdown(answer)
            
            # æ˜¾ç¤ºç½®ä¿¡åº¦
            if relevant_docs and relevant_docs[0]['score'] > 0:
                confidence = min(0.7 + (relevant_docs[0]['score'] * 0.1), 0.95)
                st.progress(confidence, text=f"å›ç­”ç½®ä¿¡åº¦: {confidence:.0%}")
            else:
                st.progress(0.5, text="å›ç­”ç½®ä¿¡åº¦: 50% (åŸºäºé€šç”¨çŸ¥è¯†)")
            
            # æ·»åŠ åˆ°æŸ¥è¯¢å†å²
            st.session_state.search_history[-1]['answer'] = answer[:100] + "..." if len(answer) > 100 else answer
            
        except Exception as e:
            st.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            st.info("ä½¿ç”¨å¤‡ç”¨å›ç­”æ¨¡å¼...")
            
            # å¤‡ç”¨å›ç­”
            if "ç³–å°¿ç—…" in question:
                answer = "ç³–å°¿ç—…å¸¸è§ç—‡çŠ¶åŒ…æ‹¬å¤šé¥®ã€å¤šå°¿ã€å¤šé£Ÿã€ä½“é‡ä¸‹é™ç­‰ã€‚å»ºè®®å®šæœŸæ£€æŸ¥è¡€ç³–ï¼Œä¿æŒå¥åº·é¥®é£Ÿå’Œé€‚é‡è¿åŠ¨ã€‚"
            elif "é«˜è¡€å‹" in question:
                answer = "é«˜è¡€å‹é¢„é˜²åŒ…æ‹¬ä½ç›é¥®é£Ÿã€å®šæœŸé”»ç‚¼ã€æ§åˆ¶ä½“é‡ã€æˆ’çƒŸé™é…’ã€‚å¦‚æœ‰å¤´ç—›ã€å¤´æ™•ç­‰ç—‡çŠ¶åº”åŠæ—¶å°±åŒ»ã€‚"
            elif "æ„Ÿå†’" in question:
                answer = "æ„Ÿå†’ç—‡çŠ¶åŒ…æ‹¬æµé¼»æ¶•ã€å’³å—½ã€å–‰å’™ç—›ã€æ‰“å–·åšç­‰ã€‚å»ºè®®å¤šä¼‘æ¯ã€å¤šå–æ°´ï¼Œå¿…è¦æ—¶å¯æœç”¨éå¤„æ–¹è¯ç‰©ã€‚"
            else:
                answer = f"å…³äº'{question}'ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚ä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼æœ‰åŠ©äºé¢„é˜²å¤šç§ç–¾ç—…ã€‚"
            
            st.success(answer)

elif search_button:
    st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜åå†ç‚¹å‡»æŸ¥è¯¢ã€‚")

# ========== æŸ¥è¯¢å†å² ==========
if st.session_state.search_history:
    st.subheader("ğŸ“œ æŸ¥è¯¢å†å²")
    with st.expander("æŸ¥çœ‹å†å²æŸ¥è¯¢", expanded=False):
        for i, record in enumerate(reversed(st.session_state.search_history[-5:]), 1):
            st.write(f"{i}. **{record['question']}**")
            if 'answer' in record:
                st.caption(f"  å›ç­”æ‘˜è¦: {record['answer']}")
            st.divider()

# ========== ç³»ç»Ÿä¿¡æ¯ ==========
st.markdown("---")
col_info1, col_info2, col_info3 = st.columns(3)

with col_info1:
    st.caption("ğŸ§  **æ™ºèƒ½æ£€ç´¢**")
    st.caption("åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å‘é‡æ£€ç´¢")

with col_info2:
    st.caption("ğŸ¥ **ä¸“ä¸šé¢†åŸŸ**")
    st.caption("åŒ»ç–—å¥åº·çŸ¥è¯†é—®ç­”")

with col_info3:
    st.caption("ğŸ”„ **å®æ—¶æ›´æ–°**")
    st.caption("æ”¯æŒåŠ¨æ€åŠ è½½çŸ¥è¯†åº“")

st.markdown("---")
st.caption("åŒ»ç–—RAGç³»ç»Ÿ v1.0 | æ•°æ®æ¥æº: GraphRAG-BenchmarkåŒ»ç–—æ•°æ®é›† | ä»…ä¾›æ•™å­¦æ¼”ç¤ºä½¿ç”¨")

# è°ƒè¯•ä¿¡æ¯
if st.sidebar.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", value=False):
    st.sidebar.subheader("ğŸ”§ è°ƒè¯•ä¿¡æ¯")
    st.sidebar.write(f"Pythonè·¯å¾„: {sys.executable}")
    st.sidebar.write(f"å½“å‰ç›®å½•: {current_dir}")
    st.sidebar.write(f"å¯¼å…¥æ–¹å¼: {IMPORT_METHOD}")
    st.sidebar.write(f"æœç´¢å†å²: {len(st.session_state.search_history)} æ¡")
    st.sidebar.write(f"å®é™…è¯­æ–™åº“æ–‡æ¡£: {actual_doc_count}")
    st.sidebar.write(f"å®é™…é—®é¢˜é›†é—®é¢˜: {actual_question_count}")